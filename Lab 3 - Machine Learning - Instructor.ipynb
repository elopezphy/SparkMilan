{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Lab 3 - Spark MLlib\n\n##### \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E\"\n-Tom M. Mitchell\n\n#### Machine Learning - the science of getting computers to act without being explicitly programmed\n\nMLlib is Spark\u2019s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. It consists of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering (this example!), dimensionality reduction, as well as lower-level optimization primitives and higher-level pipeline APIs.\n\nIt divides into two packages:\n- spark.mllib contains the original API built on top of RDDs.\n- spark.ml provides higher-level API built on top of DataFrames for constructing ML pipelines.\n\n\nUsing spark.ml is recommended because with DataFrames the API is more versatile and flexible. But we will keep supporting spark.mllib along with the development of spark.ml. Users should be comfortable using spark.mllib features and expect more features coming.\n\nhttp://spark.apache.org/docs/latest/mllib-guide.html\n\n## Online Purchase Recommendations\n\nLearn how to create a recommendation engine using the Alternating Least Squares algorithm in Spark's machine learning library\n\n<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/ALS.png' width=\"70%\" height=\"70%\"></img>\n\n### The data\n\nThis is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.  The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\nhttp://archive.ics.uci.edu/ml/datasets/Online+Retail\n\n<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/FullFile.png' width=\"80%\" height=\"80%\"></img>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Create an RDD from the csv data ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Download the data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2018-02-26 22:35:00--  https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7483128 (7.1M) [application/octet-stream]\nSaving to: \u2018OnlineRetail.csv.gz\u2019\n\n100%[======================================>] 7,483,128   --.-K/s   in 0.1s    \n\n2018-02-26 22:35:01 (51.3 MB/s) - \u2018OnlineRetail.csv.gz\u2019 saved [7483128/7483128]\n\n"
                }
            ], 
            "source": "!rm 'OnlineRetail.csv.gz' -f\n!wget https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz"
        }, 
        {
            "source": "### Put the csv into an RDD (at first, each row in the RDD is a string which correlates to a line in the csv)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "loadRetailData = sc.textFile(\"OnlineRetail.csv.gz\")\nprint loadRetailData.take(2)"
        }, 
        {
            "source": "## Prepare and shape the data:  \"80% of a Data Scientists  job\"", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Remove the header from the RDD and split the string in each row by comma", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[u'536365', u'85123A', u'WHITE HANGING HEART T-LIGHT HOLDER', u'6', u'12/1/10 8:26', u'2.55', u'17850', u'United Kingdom'], [u'536365', u'71053', u'WHITE METAL LANTERN', u'6', u'12/1/10 8:26', u'3.39', u'17850', u'United Kingdom']]\n"
                }
            ], 
            "source": "header = loadRetailData.first()\nloadRetailData = loadRetailData.filter(lambda line: line != header).\\\n                            map(lambda l: l.split(\",\"))\n\nprint loadRetailData.take(2)"
        }, 
        {
            "source": "##### NOTE:  The original file at UCI's Machine Learning Repository has commas in the product description.  Those have been removed to expediate the lab.\n#### Only keep rows that have a purchase quantity of greater than 0, a customerID not equal to 0, and a non blank stock code after removing non-numeric characters.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[u'536365', u'85123A', u'WHITE HANGING HEART T-LIGHT HOLDER', u'6', u'12/1/10 8:26', u'2.55', u'17850', u'United Kingdom'], [u'536365', u'71053', u'WHITE METAL LANTERN', u'6', u'12/1/10 8:26', u'3.39', u'17850', u'United Kingdom']]\n"
                }
            ], 
            "source": "import re\n\nloadRetailData = loadRetailData.filter(lambda l: int(l[3]) > 0\\\n                                and len(re.sub(\"\\D\", \"\", l[1])) != 0 \\\n                                and len(l[6]) != 0)\n\nprint loadRetailData.take(2)"
        }, 
        {
            "source": "#### Map each line to a row and create a data frame ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- country: string (nullable = true)\n |-- custId: long (nullable = true)\n |-- description: string (nullable = true)\n |-- inv: long (nullable = true)\n |-- invDate: string (nullable = true)\n |-- price: double (nullable = true)\n |-- quant: long (nullable = true)\n |-- stockCode: long (nullable = true)\n\nNone\n          country  custId                         description     inv  \\\n0  United Kingdom   17850  WHITE HANGING HEART T-LIGHT HOLDER  536365   \n1  United Kingdom   17850                 WHITE METAL LANTERN  536365   \n\n        invDate  price  quant  stockCode  \n0  12/1/10 8:26   2.55      6      85123  \n1  12/1/10 8:26   3.39      6      71053  \n"
                }
            ], 
            "source": "from pyspark.sql import SQLContext, Row\nsqlContext = SQLContext(sc)\n\n#Convert each line to a Row.\nloadRetailData = loadRetailData.map(lambda l: Row(inv=int(l[0]),\\\n                                    stockCode=int(re.sub(\"\\D\", \"\", l[1])),\\\n                                    description=l[2],\\\n                                    quant=int(l[3]),\\\n                                    invDate=l[4],\\\n                                    price=float(l[5]),\\\n                                    custId=int(l[6]),\\\n                                    country=l[7]))\n\n# Infer the schema, and register the DataFrame as a table.\nretailDf = sqlContext.createDataFrame(loadRetailData)\nprint retailDf.printSchema()\n\nretailDf.registerTempTable(\"retailPurchases\")\nprint sqlContext.sql(\"SELECT * FROM retailPurchases limit 2\").toPandas()"
        }, 
        {
            "source": "#### Keep only the data we need (custId, stockCode, and rank)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Row(custId=12838, stockCode=22941, purch=1), Row(custId=17968, stockCode=22731, purch=1)]\n"
                }
            ], 
            "source": "query = \"\"\"\nSELECT \n    custId, stockCode, 1 as purch\nFROM \n    retailPurchases \ngroup \n    by custId, stockCode\"\"\"\nretailDf = sqlContext.sql(query)\n\nprint retailDf.take(2)"
        }, 
        {
            "source": "### Randomly split the data into a testing set (10% of the data), a cross validation set (10% of the data) a training set (80% of the data)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Row(custId=17968, stockCode=22731, purch=1), Row(custId=17897, stockCode=84558, purch=1)]\n[Row(custId=12838, stockCode=22941, purch=1), Row(custId=13468, stockCode=21231, purch=1)]\n[Row(custId=16210, stockCode=20977, purch=1), Row(custId=13090, stockCode=22617, purch=1)]\n"
                }
            ], 
            "source": "testDf, cvDf, trainDf = retailDf.randomSplit([.1,.1,.8],1)\n\nprint trainDf.take(2)\nprint cvDf.take(2)\nprint testDf.take(2)"
        }, 
        {
            "source": "## Build recommendation models\n\n#### Use training DF to train a model with Alternating Least Squares \nLatent Factors / rank<br>\nThe number of columns in the user-feature and product-feature matricies)<br>\nIterations / maxIter<br>\nThe number of factorization runs<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "The models has been trained\n"
                }
            ], 
            "source": "from pyspark.ml.recommendation import ALS\n\nals1 = ALS(rank=3, maxIter=15,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel1 = als1.fit(trainDf)\n\nals2 = ALS(rank=15, maxIter=3,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel2 = als2.fit(trainDf)\n\nals3 = ALS(rank=15, maxIter=15,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel3 = als3.fit(trainDf)\n\nprint \"The models has been trained\""
        }, 
        {
            "source": "## Test the models\n\nUse the models to predict what the user will rate a certain item.  The closer our model to 1 that our model rates an item a user has already purchased, the better.\n\n#### Evaluate the model with the cross validation dataframe by using the transform function.\n\nSome of the users or purchases in the cross validation data may not have been in the training data.  Let's remove the ones that are not.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "25869\n25844\n"
                }
            ], 
            "source": "from pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import BooleanType\ncustomers = set(trainDf.map(lambda line: line.custId).collect())\nstock = set(trainDf.map(lambda line: line.stockCode).collect())\n\nprint cvDf.count()\ncvDf = cvDf.rdd.filter(lambda line: line.stockCode in stock and\\\n                                           line.custId in customers).toDF()\nprint cvDf.count()"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Row(custId=14286, stockCode=20831, purch=1, prediction=0.01889532431960106), Row(custId=13949, stockCode=20831, purch=1, prediction=0.018036123365163803)]\n[Row(custId=14286, stockCode=20831, purch=1, prediction=0.04725762456655502), Row(custId=13949, stockCode=20831, purch=1, prediction=0.03975377976894379)]\n[Row(custId=14286, stockCode=20831, purch=1, prediction=0.0679328441619873), Row(custId=13949, stockCode=20831, purch=1, prediction=0.06638696044683456)]\n"
                }
            ], 
            "source": "predictions1 = model1.transform(cvDf)\npredictions2 = model2.transform(cvDf)\npredictions3 = model3.transform(cvDf)\n\nprint predictions1.take(2)\nprint predictions2.take(2)\nprint predictions3.take(2)"
        }, 
        {
            "source": "#### Calculate and print the Mean Squared Error.   For all ratings, subtract the prediction from the actual purchase (1), square the result, and take the mean of all of the squared differences.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Mean squared error = 0.7388 for our first model\nMean squared error = 0.7003 for our second model\nMean squared error = 0.6691 for our second model\n"
                }
            ], 
            "source": "meanSquaredError1 = predictions1.map(lambda line: (line.purch - line.prediction)**2).mean()\nmeanSquaredError2 = predictions2.map(lambda line: (line.purch - line.prediction)**2).mean()\nmeanSquaredError3 = predictions3.map(lambda line: (line.purch - line.prediction)**2).mean()\n    \nprint 'Mean squared error = %.4f for our first model' % meanSquaredError1\nprint 'Mean squared error = %.4f for our second model' % meanSquaredError2\nprint 'Mean squared error = %.4f for our second model' % meanSquaredError3"
        }, 
        {
            "source": "#### Confirm the model by testing it with the test data and the best hyperparameters found during cross validation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Mean squared error = 0.6677 for our best model\n"
                }
            ], 
            "source": "filteredTestDf = testDf.rdd.filter(lambda line: line.stockCode in stock and\\\n                                              line.custId in customers).toDF()\npredictions4 = model3.transform(filteredTestDf)\nmeanSquaredError4 = predictions4.map(lambda line: (line.purch - line.prediction)**2).mean()\n    \nprint 'Mean squared error = %.4f for our best model' % meanSquaredError4"
        }, 
        {
            "source": "# Implement the model\n\nUse the best model to predict items the user will be interested in.\n\nFirst, create a dataframe in which each row has the user id and an item id.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Row(stockCode=85231, custId=15544), Row(stockCode=22631, custId=15544), Row(stockCode=21631, custId=15544), Row(stockCode=84231, custId=15544), Row(stockCode=22431, custId=15544)]\n"
                }
            ], 
            "source": "from pyspark.sql.functions import lit\n\nstock15544 = set(trainDf.filter(trainDf['custId'] == 15544).map(lambda line: line.stockCode).collect())\n\nuserItems = trainDf.select(\"stockCode\").distinct().\\\n            withColumn('custId', lit(15544)).\\\n            rdd.filter(lambda line: line.stockCode not in stock15544).toDF()\n\nprint userItems.take(5)"
        }, 
        {
            "source": "Use 'transform' to rate each item.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Row(stockCode=20831, custId=15544, prediction=0.004703751299530268), Row(stockCode=21031, custId=15544, prediction=0.010227371007204056), Row(stockCode=21231, custId=15544, prediction=0.058025069534778595), Row(stockCode=21631, custId=15544, prediction=-0.02571825124323368), Row(stockCode=22031, custId=15544, prediction=0.05723518133163452)]\n"
                }
            ], 
            "source": "userItems = model3.transform(userItems)\n\nprint userItems.take(5)"
        }, 
        {
            "source": "Print the top 5 recommendations.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Row(stockCode=84997, custId=15544, prediction=0.5976683497428894), Row(stockCode=21213, custId=15544, prediction=0.5788490772247314), Row(stockCode=16161, custId=15544, prediction=0.5184972286224365), Row(stockCode=22138, custId=15544, prediction=0.505846381187439), Row(stockCode=22090, custId=15544, prediction=0.49148523807525635)]\n"
                }
            ], 
            "source": "print userItems.sort(\"prediction\",ascending=False).take(5)"
        }, 
        {
            "source": "Let's look up this user and the recommended product ID's in the excel file...\n\n<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/user.png' width=\"80%\" height=\"80%\"></img>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This user seems to have purchased a lot of childrens gifts and some holiday items.  The recomendation engine we created suggested some items along these lines\n\n##### The ALS algorithm uses some randomness, so the recommendations yours produces may be different than these.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "                           description\n0                   WRAP ENGLISH ROSE \n1    CHILDRENS CUTLERY POLKADOT GREEN \n2              PAPER BUNTING RETROSPOT\n3                     WRAP  PINK FLOCK\n4                    WRAP BAD HAIR DAY\n5    RED 3 PIECE RETROSPOT CUTLERY SET\n6    PINK 3 PIECE POLKADOT CUTLERY SET\n7          PACK OF 72 SKULL CAKE CASES\n8        BAKING SET 9 PIECE RETROSPOT \n9      CHILDRENS CUTLERY POLKADOT PINK\n10               WRAP SUKI AND FRIENDS\n11  GREEN 3 PIECE POLKADOT CUTLERY SET\n12     CHILDRENS CUTLERY POLKADOT BLUE\n13   BLUE 3 PIECE POLKADOT CUTLERY SET\n14    CHILDRENS CUTLERY RETROSPOT RED \n"
                }
            ], 
            "source": "query = \"\"\"\nSELECT \n    distinct description \nFROM \n    retailPurchases \nWHERE \n    stockCode in (84997,21213,16161,22138,22090)\n\"\"\"\nitems = sqlContext.sql(query)\nprint items.toPandas()"
        }, 
        {
            "source": "##### Data Citation\nDaqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197\u00e2\u20ac\u201c208, 2012 (Published online before print: 27 August 2012. doi: 10.1057/dbm.2012.17).", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 1.6 (Unsupported)", 
            "name": "python2", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}